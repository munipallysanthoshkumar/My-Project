# -*- coding: utf-8 -*-
"""Data_Loading_Binary classes.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Dng5g_FiO0H_eSt9ACTYjKIc2fU6CGUX

# 1. Data Preparation

### Import libraries
"""

!pip -q install lime

import os
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
import tensorflow as tf
from sklearn.utils.class_weight import compute_class_weight
import hashlib
from PIL import Image
from collections import defaultdict
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras import layers, models
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve
from tensorflow.keras.preprocessing import image
from lime import lime_image
from skimage.segmentation import mark_boundaries
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing import image
from google.colab import files
import os
from sklearn.utils.class_weight import compute_class_weight

np.random.seed(12049)

from google.colab import drive
drive.mount('/content/drive')

"""# Data Loading / Setup"""

drive.mount('/content/drive')

project_path = "/content/drive/MyDrive/knee_oa_project"
zip_path = "/content/drive/MyDrive/KneeXrayData.zip"
extract_path = f"{project_path}/dataset"

# make folders if not there
os.makedirs(project_path, exist_ok=True)
os.makedirs(extract_path, exist_ok=True)

print("Project folder:", project_path)
print("Extract folder:", extract_path)

# unzip only if dataset folder looks empty
if not os.path.exists(zip_path):
    raise FileNotFoundError("KneeXrayData.zip not found in Drive. Please upload it first.")

if len(os.listdir(extract_path)) == 0:
    print("Unzipping dataset...")
    !unzip -q "{zip_path}" -d "{extract_path}"
    print("Done.")
else:
    print("Dataset already extracted. Skipping unzip.")

# paths for train/val/test (based on your dataset structure)
train_dir = f"{extract_path}/KneeXrayData/ClsKLData/kneeKL224/train"
val_dir   = f"{extract_path}/KneeXrayData/ClsKLData/kneeKL224/val"
test_dir  = f"{extract_path}/KneeXrayData/ClsKLData/kneeKL224/test"

print("\nTrain:", train_dir)
print("Val  :", val_dir)
print("Test :", test_dir)

# quick check
for p in [train_dir, val_dir, test_dir]:
    if not os.path.exists(p):
        print("Missing:", p)
    else:
        print("Found:", p)

"""# Data Exploration"""

# class names (KL grades)
class_names = ['Healthy', 'Doubtful', 'Minimal', 'Moderate', 'Severe']

def get_class_counts(data_path, name):
    print(f"\n{name} data:")
    total = 0
    counts = {}

    for i in range(len(class_names)):
        class_path = os.path.join(data_path, str(i))
        num_images = len(os.listdir(class_path))
        counts[i] = num_images
        total += num_images

    for k, v in counts.items():
        pct = round((v / total) * 100, 2)
        print(f"Grade {k} ({class_names[k]}): {v} images ({pct}%)")

    return counts

# get counts
train_counts = get_class_counts(train_dir, "Train")
val_counts   = get_class_counts(val_dir, "Validation")
test_counts  = get_class_counts(test_dir, "Test")

plt.figure(figsize=(8,5))
plt.bar(class_names, train_counts.values())
plt.title("Class Distribution - Training Data")
plt.xlabel("Knee OA Grade")
plt.ylabel("Number of Images")
plt.xticks(rotation=15)
plt.show()

"""# Show sample images from each class (train)"""

def show_sample_images(data_path, samples_per_class=3):
    for i in range(len(class_names)):
        class_path = os.path.join(data_path, str(i))
        images = os.listdir(class_path)

        print(f"\nSample images - Grade {i} ({class_names[i]})")
        plt.figure(figsize=(10,3))

        for j in range(min(samples_per_class, len(images))):
            img_path = os.path.join(class_path, images[j])
            img = tf.keras.preprocessing.image.load_img(img_path)

            plt.subplot(1, samples_per_class, j + 1)
            plt.imshow(img, cmap="gray")
            plt.axis("off")

        plt.show()

# show samples from training set
show_sample_images(train_dir)

"""# Data Quality & Integrity Checks"""

def find_duplicates(dataset_dir, name):
    print(f"\nChecking exact duplicates in {name}...")
    hashes = defaultdict(list)

    for class_folder in os.listdir(dataset_dir):
        class_path = os.path.join(dataset_dir, class_folder)
        if not os.path.isdir(class_path):
            continue

        for fname in os.listdir(class_path):
            fpath = os.path.join(class_path, fname)
            if not os.path.isfile(fpath):
                continue

            with open(fpath, "rb") as f:
                h = hashlib.md5(f.read()).hexdigest()
            hashes[h].append(fpath)

    duplicates = {h: files for h, files in hashes.items() if len(files) > 1}

    if len(duplicates) == 0:
        print("No exact duplicates found.")
    else:
        print("Duplicate groups found:", len(duplicates))
        # show first few groups
        shown = 0
        for h, files in duplicates.items():
            print("\nHash:", h)
            for f in files:
                print(" -", f)
            shown += 1
            if shown == 3:
                break

    return duplicates


train_dups = find_duplicates(train_dir, "Train")
val_dups   = find_duplicates(val_dir, "Validation")
test_dups  = find_duplicates(test_dir, "Test")

def find_cross_duplicates(dir1, dir2, name1, name2):
    print(f"\nChecking cross-duplicates between {name1} and {name2}...")

    hashes1 = {}
    for class_folder in os.listdir(dir1):
        class_path = os.path.join(dir1, class_folder)
        if not os.path.isdir(class_path):
            continue

        for fname in os.listdir(class_path):
            fpath = os.path.join(class_path, fname)
            if not os.path.isfile(fpath):
                continue

            with open(fpath, "rb") as f:
                h = hashlib.md5(f.read()).hexdigest()
            hashes1[h] = fpath

    cross = []
    for class_folder in os.listdir(dir2):
        class_path = os.path.join(dir2, class_folder)
        if not os.path.isdir(class_path):
            continue

        for fname in os.listdir(class_path):
            fpath = os.path.join(class_path, fname)
            if not os.path.isfile(fpath):
                continue

            with open(fpath, "rb") as f:
                h = hashlib.md5(f.read()).hexdigest()

            if h in hashes1:
                cross.append((hashes1[h], fpath))

    if len(cross) == 0:
        print("No cross-duplicates found.")
    else:
        print("Cross-duplicates found:", len(cross))
        for a, b in cross[:10]:
            print(" -", a)
            print("   ", b)

    return cross


train_val_cross = find_cross_duplicates(train_dir, val_dir, "Train", "Validation")
train_test_cross = find_cross_duplicates(train_dir, test_dir, "Train", "Test")
val_test_cross = find_cross_duplicates(val_dir, test_dir, "Validation", "Test")

"""# Binary Class Mapping"""

binary_map = {0: 0, 1: 0, 2: 1, 3: 1, 4: 1}
binary_class_names = ["Non-OA", "OA"]

print("Binary mapping:", binary_map)
print("Binary classes:", binary_class_names)

def get_binary_counts(data_dir, name):
    total = 0
    bin_counts = {0: 0, 1: 0}

    for grade in range(5):
        class_path = os.path.join(data_dir, str(grade))
        if not os.path.exists(class_path):
            raise FileNotFoundError(f"Missing folder: {class_path}")

        n = len([f for f in os.listdir(class_path) if os.path.isfile(os.path.join(class_path, f))])
        bin_label = binary_map[grade]
        bin_counts[bin_label] += n
        total += n

    print(f"\n{name} Binary Distribution:")
    for k in [0, 1]:
        pct = round((bin_counts[k] / total) * 100, 2) if total > 0 else 0
        print(f"{binary_class_names[k]} ({k}): {bin_counts[k]} images ({pct}%)")

    print("Total:", total)
    return bin_counts

train_bin_counts = get_binary_counts(train_dir, "Train")
val_bin_counts   = get_binary_counts(val_dir, "Validation")
test_bin_counts  = get_binary_counts(test_dir, "Test")

labels = sorted(set(binary_map.values()))
print("Unique binary labels from mapping:", labels)

assert labels == [0, 1], "Binary mapping is incorrect. Expected labels [0,1]."
print("Mapping sanity check passed.")

IMG_SIZE = (224, 224)
BATCH_SIZE = 32

train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=15,
    width_shift_range=0.05,
    height_shift_range=0.05,
    zoom_range=0.05,
    horizontal_flip=True,
    fill_mode="nearest"
)

val_test_datagen = ImageDataGenerator(
    rescale=1./255
)

print("IMG_SIZE:", IMG_SIZE, "| BATCH_SIZE:", BATCH_SIZE)

IMG_SIZE = (224, 224)
BATCH_SIZE = 32

train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=15,
    width_shift_range=0.05,
    height_shift_range=0.05,
    zoom_range=0.05,
    horizontal_flip=True,
    fill_mode="nearest"
)

val_test_datagen = ImageDataGenerator(
    rescale=1./255
)

print("IMG_SIZE:", IMG_SIZE, "| BATCH_SIZE:", BATCH_SIZE)

train_gen_5 = train_datagen.flow_from_directory(
    train_dir,
    target_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    class_mode="sparse",
    shuffle=True,
    seed=42
)

val_gen_5 = val_test_datagen.flow_from_directory(
    val_dir,
    target_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    class_mode="sparse",
    shuffle=False
)

test_gen_5 = val_test_datagen.flow_from_directory(
    test_dir,
    target_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    class_mode="sparse",
    shuffle=False
)

def gen_to_binary_dataset(gen5):
    while True:
        x, y = next(gen5)
        y = y.astype(int)
        y_bin = np.vectorize(binary_map.get)(y)  # map 0-4 -> 0/1
        yield x, y_bin.astype(np.float32)

output_signature = (
    tf.TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32),
    tf.TensorSpec(shape=(None,), dtype=tf.float32)
)

train_ds = tf.data.Dataset.from_generator(lambda: gen_to_binary_dataset(train_gen_5),
                                          output_signature=output_signature).prefetch(tf.data.AUTOTUNE)

val_ds   = tf.data.Dataset.from_generator(lambda: gen_to_binary_dataset(val_gen_5),
                                          output_signature=output_signature).prefetch(tf.data.AUTOTUNE)

test_ds  = tf.data.Dataset.from_generator(lambda: gen_to_binary_dataset(test_gen_5),
                                          output_signature=output_signature).prefetch(tf.data.AUTOTUNE)

print("Binary datasets created: train_ds, val_ds, test_ds")

xb, yb = next(iter(train_ds))
print("X batch shape:", xb.shape)
print("y batch shape:", yb.shape)
print("Unique labels in this batch:", np.unique(yb))

assert xb.shape[1:] == (224, 224, 3), "Image shape mismatch"
assert set(np.unique(yb)).issubset({0.0, 1.0}), "Binary labels should be 0/1 only"
print("Batch sanity check passed.")